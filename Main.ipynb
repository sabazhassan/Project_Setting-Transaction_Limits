{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "%run import_library.ipynb\n",
    "%run data_preprocessing.ipynb\n",
    "%run Evaluation_functions.ipynb\n",
    "%run categorical_data_encoding.ipynb\n",
    "%run OneVsRestLightGBMWithCustomizedLoss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demograph_train = pd.read_csv(\"demographics_train.csv\")\n",
    "exposure_train = pd.read_csv(\"exposure_train.csv\")\n",
    "transfers_train = pd.read_csv(\"transfers_train.csv\")\n",
    "transfers_train = transfers_train.dropna()\n",
    "demograph_train.head()\n",
    "print(transfers_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read testing data\n",
    "demograph_test = pd.read_csv(\"demographics_test.csv\")\n",
    "exposure_test = pd.read_csv(\"exposure_test.csv\")\n",
    "transfers_test = pd.read_csv(\"transfers_test.csv\")\n",
    "transfers_test = transfers_test.dropna()\n",
    "demograph_test.head()\n",
    "print(transfers_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target class distribution\n",
    "demograph_train.groupby(\"level\").count()\n",
    "sns.countplot(demograph_train[\"level\"])\n",
    "sns.countplot(demograph_train[\"COUNTRY_CODE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations\n",
    "#Data is very skewed to US with only a few data points from other countries\n",
    "#The data is of highly imbalanced classes with majority class being level b, d and c\n",
    "#Models to test: Classifications model like Logistic regression, Random forest, Boosting algorithms, Tree based algorithms etc\n",
    "\n",
    "#Data Cleanup\n",
    "#1. Demographic data\n",
    "#a) Age - calculate age from date\n",
    "#b) Location - cleanup countries and states\n",
    "#c) Occupation\n",
    "#d) create column days to verify\n",
    "#e) Drop rows that have NAs in country code and Birth year# Clean Demographic Train data\n",
    "\n",
    "demograph_train_cleaned = cleanup_demographics(demograph_train)\n",
    "demo_onehot_columns = [ 'COUNTRY_CODE',  'OCC_CAT']\n",
    "demograph_train_cleaned = one_hot_ecoding(demograph_train_cleaned, demo_onehot_columns)\n",
    "demograph_train_cleaned = demograph_train_cleaned.set_index(\"EXCHANGE_ACCOUNT_ID\")\n",
    "len(demograph_train_cleaned.columns)\n",
    "\n",
    "# Clean Demographic Test data\n",
    "demograph_test_cleaned = cleanup_demographics(demograph_train)\n",
    "demograph_test_cleaned = one_hot_ecoding(demograph_test_cleaned, demo_onehot_columns)\n",
    "demograph_test_cleaned = demograph_test_cleaned.set_index(\"EXCHANGE_ACCOUNT_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Exposure data\n",
    "#a) Encode categorical data\n",
    "#b) Aggregate exposure amounts on account_id and category    \n",
    "exposure_one_hot_columns = 'cluster_category'\n",
    "exposure_train_encoded = one_hot_ecoding(exposure_train, exposure_one_hot_columns)\n",
    "exposure_train_agg = exposure_agg(exposure_train_encoded)\n",
    "\n",
    "exposure_test_encoded = one_hot_ecoding(exposure_test, exposure_one_hot_columns)\n",
    "exposure_test_agg = exposure_agg(exposure_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Transfers data\n",
    "#a) Cleanup data \n",
    "#b) Encode categorical data\n",
    "#c) Aggregate transfer amounts on account_id, type, tx_time and currency \n",
    "transfers_train_cleaned = cleanup_transfers(transfers_train)\n",
    "transfers_feature_cols = [\"TX_YEAR\", 'CURRENCY', \"TYPE\"]\n",
    "transfers_train_encoded = one_hot_ecoding(transfers_train_cleaned, transfers_feature_cols)\n",
    "transfers_train_agg = transfer_agg(transfers_train_encoded)\n",
    "\n",
    "transfers_test_cleaned = cleanup_transfers(transfers_test)\n",
    "transfers_test_encoded = one_hot_ecoding(transfers_test_cleaned, transfers_feature_cols)\n",
    "transfers_test_agg = transfer_agg(transfers_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dataframes\n",
    "Transaction_data_train = pd.merge(exposure_train_agg, transfers_train_agg, how='outer',left_index=True, right_index=True).fillna(0)\n",
    "Transaction_data_test = pd.merge(exposure_test_agg, transfers_test_agg, how='outer',left_index=True, right_index=True).fillna(0)\n",
    "training_data = pd.merge(demograph_train_cleaned, Transaction_data_train, how='inner',left_index=True, right_index=True).reset_index()\n",
    "test_data = pd.merge(demograph_test_cleaned, Transaction_data_test, how='inner',left_index=True, right_index=True).reset_index()\n",
    "col_to_drop = ['EXCHANGE_ACCOUNT_ID', 'CREATED_AT', 'FIRST_VERIFIED_AT', 'STATE_CODE', 'BIRTH_YEAR', 'OCCUPATION',\n",
    "  'AGE_GROUPS', 'DAYS_TO_VERIFY_GROUPS', 'STATE_CODE_INT', 'COUNTRY_CODE_INT','TX_TIME']\n",
    "training_data = training_data.drop(col_to_drop, axis=1)\n",
    "test_data = test_data.drop(col_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELS\n",
    "# Define independent features\n",
    "target = ['level']\n",
    "features  =  list(training_data.columns)\n",
    "features.remove('level')\n",
    "training_data[features] = training_data[features].applymap(np.int64)\n",
    "\n",
    "y = training_data[target]\n",
    "X = training_data[features]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "dict_classifiers = {\n",
    "    \"Logreg\": LogisticRegression(solver='lbfgs'),\n",
    "    \"NN\": KNeighborsClassifier(),\n",
    "    #\"LinearSVM\": SVC(probability=True, kernel='linear'), #class_weight='balanced'\n",
    "    \"LGB\": lgb.LGBMClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"NB\": GaussianNB()\n",
    "}\n",
    "classification_metrics = displaymetrics(dict_classifiers, X_train, X_val,y_train, y_val)\n",
    "classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state= 42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_val_pred = rf_classifier.predict(X_val)\n",
    "pred_accuracy_score = accuracy_score(y_val, y_val_pred)\n",
    "pred_recall_score = recall_score(y_val, y_val_pred, average='macro')\n",
    "print('Prediction accuracy', pred_accuracy_score,' recall ', pred_recall_score)\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "plot_confusion_matrix(rf_classifier,X_val, y_val)\n",
    "plt.grid(False)\n",
    "\n",
    "\n",
    "# Find feature importance through Random forest classifier\n",
    "features = X_train.columns\n",
    "importances = rf_classifier.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# customized number \n",
    "num_features = 10 \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "# only plot the customized number of features\n",
    "plt.barh(range(num_features), importances[indices[-num_features:]], color='b', align='center')\n",
    "plt.yticks(range(num_features), [features[i] for i in indices[-num_features:]])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "relevant_features = [features[i] for i in indices[-num_features:]]\n",
    "relevant_features\n",
    "\n",
    "X_train = X_train[relevant_features]\n",
    "X_val = X_val[relevant_features]\n",
    "\n",
    "\n",
    "clf_lgb = lgb.LGBMClassifier()\n",
    "clf_lgb.fit(X_train, y_train)\n",
    "y_val_pred = clf_lgb.predict(X_val)\n",
    "pred_accuracy_score = accuracy_score(y_val, y_val_pred)\n",
    "pred_recall_score = recall_score(y_val, y_val_pred, average='macro')\n",
    "print('Prediction accuracy', pred_accuracy_score,' recall ', pred_recall_score)\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "plot_confusion_matrix(clf_lgb,X_val, y_val)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purpose of improving the models, we will try a few techniques on the LGBClassifer model as it has a better recall and almost same accuracy as Randomforest\n",
    "#As we can see the data is a bit challenging to classify correctly majorily due to the imbalanced classes. We will try techniques and see if this can improve the model like:\n",
    "#Resample Classes - One approach to addressing the problem of class imbalance is to randomly resample the training dataset. The two main approaches to randomly resampling an imbalanced dataset are to delete examples from the majority class, called undersampling, and to duplicate examples from the minority class, called oversampling. Since, We donot want to lose more data so we will try to over-sample the minority class and see if that would improve the model\n",
    "#Add class weights - Give minoprity class more weights than the majority class\n",
    "#OneOverRestclassifiers - In this aaproach we fit one classifier per class. For each classifier, the class is fitted against all the other classes. This startegy is good for cases where we need interpretability as each class is represented by one and only one classifier which makes it easy to gain knowledge about the class by inspecting its corresponding classifier.\n",
    "#1. Over-sampling the minority class\n",
    "print(\"Training and validation data before sampling: \", X_train.shape, y_train.shape)\n",
    "\n",
    "# using SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy ='minority', k_neighbors =2)\n",
    "x_sm, y_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Training and validation data after sampling: \", x_sm.shape, y_sm.shape)\n",
    "\n",
    "# Using the best model from the group\n",
    "dict_classifiers = {\"LGB\": lgb.LGBMClassifier()}\n",
    "classification_metrics_resampled =  displaymetrics(dict_classifiers, x_sm, X_val,y_sm, y_val)\n",
    "classification_metrics_resampled\n",
    "\n",
    "##2. Adding class weights\n",
    "dict_classifiers = {\"LGB\": lgb.LGBMClassifier(class_weight = 'balanced')}\n",
    "classification_metrics_resampled_weights =  displaymetrics(dict_classifiers, X_train, X_val, y_train, y_val)\n",
    "classification_metrics_resampled_weights\n",
    "\n",
    "#3. OneoverRestclassifiers\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr = OneVsRestClassifier(lgb.LGBMClassifier( boosting_type='gbdt',  num_leaves=30, max_depth=5, learning_rate=0.1, n_estimators=100, max_bin=225, \n",
    "  objective=None, min_split_gain=0, \n",
    " min_child_weight=5, \n",
    " min_child_samples=10, subsample=1, subsample_freq=1, \n",
    "colsample_bytree=1, \n",
    "reg_alpha=1, reg_lambda=0, seed=410,  silent=True))\n",
    "   \n",
    "# Fitting the model with training data\n",
    "ovr.fit(x_sm, y_sm)\n",
    "   \n",
    "# Making a prediction on the test set\n",
    "prediction = ovr.predict(X_val)\n",
    "   \n",
    "# Evaluating the model\n",
    "print(f\"Validation Set Accuracy : {accuracy_score(y_val, prediction) * 100} %\\n\\n\")\n",
    "print(f\"Classification Report : \\n\\n{classification_report(y_val, prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "# n_estimater:number of boosting iterations, \n",
    "# Note: internally, LightGBM constructs num_class * num_iterations trees for multi-class classification problems\n",
    "# n_leaves: max number of leaves in one tree\n",
    "# added L1 regularization too as the model has been over fitting for training samples\n",
    "\n",
    "\n",
    "param_set = {\n",
    " 'n_estimators':[100,150,200]\n",
    "    , 'num_leaves':range(20,60,10)\n",
    "}\n",
    "\n",
    "gsearch = GridSearchCV(estimator = lgb.LGBMClassifier( boosting_type='gbdt', multiclass = \"softmax\", num_class = 9,  num_leaves=30, max_depth=5, learning_rate=0.1, n_estimators=100, max_bin=225, \n",
    "  objective=None, min_split_gain=0, \n",
    " min_child_weight=5, \n",
    " min_child_samples=10, subsample=1, subsample_freq=1, \n",
    "colsample_bytree=1, \n",
    "reg_alpha=1, reg_lambda=0, seed=410, nthread=7, silent=True), \n",
    "param_grid = param_set, scoring='f1_macro',n_jobs=7, cv=10)\n",
    "\n",
    "lgb_model2 = gsearch.fit(x_sm, y_sm)\n",
    "lgb_model2.best_estimator_, lgb_model2.best_score_\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(lgb_model2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the saved model and use it to run on the test data provided\n",
    "#make sure features are available in the test data\n",
    "test_features = [relevant_features for relevant_features in test_data.columns if relevant_features in test_data.columns]\n",
    "test_features.remove('level')\n",
    "test_data_full = pd.DataFrame(columns = relevant_features)\n",
    "test_data_final = pd.concat([ test_data_full, test_data],  join=\"outer\").fillna(0)\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_test_pred = loaded_model.predict(test_data_final[relevant_features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
