{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_test, y_test_pred, model):\n",
    "    #Calculating and printing the f1 score \n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    print('The f1 score for the testing data:', f1_test)\n",
    "\n",
    "\n",
    "    print(\"Precision Score : \",precision_score(y_test, y_test_pred, \n",
    "\n",
    "                                               average='macro'))\n",
    "    print(\"Recall Score : \",recall_score(y_test, y_test_pred, \n",
    "\n",
    "                                               average='macro'))\n",
    "\n",
    "    pred_accuracy_score = accuracy_score(y_test, y_test_pred)\n",
    "    #pred_recall_score = recall_score(y_test, y_test_pred, average='macro')\n",
    "    print('Prediction accuracy', pred_accuracy_score,)\n",
    "\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_test, y_test_pred, labels=labeles)\n",
    "    plot_confusion_matrix(model,X_test, y_test)\n",
    "    def displaymetrics(dict_classifiers, X_train, X_test, y_train, y_test):\n",
    "    df_final = pd.DataFrame()\n",
    "    for model, model_instantiation in dict_classifiers.items():\n",
    "\n",
    "        model_fit = model_instantiation.fit(X_train, y_train)\n",
    "  \n",
    " \n",
    "        y_pred = pd.DataFrame(model_fit.predict(X_train)).reset_index(drop=True)\n",
    "        Recall_Train,Precision_Train, Accuracy_Train  = recall_score(y_train, y_pred, average='macro'), precision_score(y_train, y_pred, average='macro'),accuracy_score(y_train, y_pred)\n",
    "        y_pred = pd.DataFrame(model_fit.predict(X_test)).reset_index(drop=True)\n",
    "        Recall_Test = recall_score(y_test, y_pred, average='macro')\n",
    "        Precision_Test = precision_score(y_test, y_pred, average='macro')\n",
    "        Accuracy_Test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        cv_f1 = cross_val_score(model_instantiation, X_test, y_test, cv=30, scoring='f1_macro')\n",
    "        cv_pr = cross_val_score(model_instantiation, X_test, y_test, cv=30, scoring='precision_macro')\n",
    "        cv_re = cross_val_score(model_instantiation, X_test, y_test, cv=30, scoring='recall_macro')\n",
    "        cv_ac = cross_val_score(model_instantiation, X_test, y_test, cv=30, scoring='accuracy')\n",
    "        cv_ba = cross_val_score(model_instantiation, X_test, y_test, cv=30, scoring='balanced_accuracy')\n",
    " \n",
    "        cv_f1_m, cv_f1_std = cv_f1.mean() , cv_f1.std()\n",
    "        cv_pr_m, cv_pr_std = cv_pr.mean() , cv_pr.std()\n",
    "        cv_re_m, cv_re_std= cv_re.mean() , cv_re.std()\n",
    "        cv_ac_m, cv_ac_std = cv_ac.mean() , cv_ac.std()\n",
    "        cv_ba_m, cv_ba_std= cv_ba.mean() , cv_ba.std()\n",
    "        cv_f1, cv_pr =   (cv_f1_m, cv_f1_std), (cv_pr_m, cv_pr_std) \n",
    "        cv_re, cv_ac, cv_ba = (cv_re_m, cv_re_std), (cv_ac_m, cv_ac_std), (cv_ba_m, cv_ba_std)\n",
    "        tuples = [ cv_f1, cv_pr, cv_re, cv_ac, cv_ba]\n",
    "        tuplas = [0]*len(tuples)\n",
    "        for i in range(len(tuples)):\n",
    "            tuplas[i] = [round(x,2) for x in tuples[i]]\n",
    "        results = pd.DataFrame()\n",
    "        results['Metrics'] = [ 'Accuracy_Train', 'Precision_Train', 'Recall_Train', 'Accuracy_Test', \n",
    "                              'Precision_Test','Recall_Test', 'cv_f1score(mean, std)', \n",
    "                              'cv_precision (mean, std)', 'cv_recall (mean, std)', 'cv_accuracy (mean, std)', \n",
    "                              'cv_bal_accuracy (mean, std)']\n",
    "        results.set_index(['Metrics'], inplace=True)\n",
    "        results['Model_'+model] = [ Accuracy_Train, Precision_Train, Recall_Train, Accuracy_Test, \n",
    "                            Precision_Test, Recall_Test, tuplas[0], tuplas[1], tuplas[2], tuplas[3],\n",
    "                           tuplas[4]]\n",
    "\n",
    "        \n",
    "        df_final = pd.concat([df_final, results], axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(y,y_hat,labels):\n",
    "  print(classification_report(y,y_hat))\n",
    "  cm = confusion_matrix(y,y_hat)\n",
    "  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "  cmat = pd.DataFrame(cm)\n",
    "  cmat.columns = labels\n",
    "  cmat.set_index([pd.Index(labels)],inplace=True)\n",
    "  sns.heatmap(cmat,cmap=\"YlGnBu\", annot=True)\n",
    "  plt.title(\"Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
